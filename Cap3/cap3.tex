In this chapter we will be introducing the novel methodology for algorithm selection and performance evaluation called Instance Space Analysis (ISA), introduced in \cite{Munoz2018}. We will be showing the original definition of an instance space and the adaptation of the methodology brought up by \cite{Lorena2022} relating instance hardness.

\section{Instance spaces}

ISA is, at its core, an extension of the Algorithm Selection Problem (ASP) \cite{RICE1976}. Figure \ref{fig:ISA_flowchart} shows the ISA framework with the ASP highlighted in blue. The objective in the ASP is to automate the process of selecting algorithms based on past similar solved problems. The following sets compose the core of the ASP:

\begin{itemize}
	\item \textbf{Problem Space} $\mathcal{P}$: contains all instances of the problem being analysed;
	\item \textbf{Instance Sub-space} $\mathcal{I}$: contains a subset of instances from $\mathcal{P}$ for which the characteristics and solutions are available;
	\item \textbf{Feature Space} $\mathcal{F}$: a set of descriptive characteristics of the instances in $\mathcal{I}$. These are also known as meta-features;
	\item \textbf{Algorithm Space} $\mathcal{A}$: contains algorithms that may be used to solve the instances in $\mathcal{I}$;
	\item \textbf{Performance Space} $\mathcal{Y}$: contains the performance evaluations of the algorithms in $\mathcal{A}$ over the instances in $\mathcal{I}$;
\end{itemize}

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[node distance=4cm]
		
		% Nodes
		\node[rectangle, draw, align=center] (problem) {Problem Space \\ $z \in \mathcal{P}$};
		\node[rectangle, draw, below of=problem, align=center] (subspace) {$x \in \mathcal{I} \subset \mathcal{P}$ \\ Sub-space of \\ instances};
		\node[rectangle, draw, below of=subspace, align=center] (feature) {$f(x) \in \mathcal{F}$ \\ Feature \\ Space};
		\node[rectangle, draw, below of=feature, align=center] (instance) {$g(f(x)) \in \mathbb{R}^2$ \\ 2-D Instance \\ Space};
		\node[rectangle, draw, right of=feature, xshift=4cm, align=center] (algorithm) {$\alpha \in \mathcal{A}$ \\ Algorithm \\ Space};
		\node[rectangle, draw, above of=algorithm, align=center] (performance) {$y \in \mathcal{Y}$ \\ Performance \\ Space};
		\node[rectangle, draw, above of=performance, align=center] (footprints) {Footprints in \\ Instance Space};
		
		% Rectangle
		\begin{scope}[on background layer]
			\node[fill=blue!30, rounded corners, opacity=0.3, fit=(subspace)(feature)(algorithm)(performance), inner ysep=0.5cm, inner xsep=2.5cm] (rectangle) {};
		\end{scope}
		
		% Arrows
		\draw[->] (problem) -- node[left, align=right] {Instance selection\\or generation} (subspace);
		\draw[->] (subspace) -- node[left, align=center] {Feature selection\\$f$} (feature);
		\draw[->] (feature) -- node[left, align=right] {Dimension reduction\\and visualization} (instance);
		\draw[->] (feature) -- node[above] (alpha) {$\alpha^* = S(f(x))$} (algorithm);
		\draw[->] (instance) -| node[above, near start, align=center] {Learn selection\\mapping\\from features} node[below, near start] {$\alpha^* = S^\prime(f(x))$} ++(8cm,0) |-  (algorithm.south);
		\draw[->] (algorithm) -- node[right, align=left] {$y(\alpha, x)$ apply \\algorithm $\alpha$ \\to instance $x$} (performance);
		\draw[->] (performance) -| node[above, near start, align=center] {Select $\alpha^*$ to\\ maximize $||y||$} ++(-4cm,0) |-  (alpha.north);
		\draw[->] (footprints) -- node[above, align=center] {Infer algorithm \\performance on \\all $z \in \mathcal{P}$} (problem);
		\draw[->] (performance) -- node[right, align=left] {Define algorithm \\footprints $\varphi(y(\alpha, x))$} (footprints);
		
	\end{tikzpicture}
	\caption{ISA framework. Extracted from \cite{Munoz2018}. The ASP is highlighted in blue.} \label{fig:ISA_flowchart}
\end{figure}

The combination of tuples $(x, f(x), \alpha, y(\alpha, x))$, where $x \in \mathcal{I}$, $f(x) \in \mathcal{F}$, $\alpha \in \mathcal{A}$ and $y(\alpha, x) \in \mathcal{Y}$, composes a meta-dataset $\mathcal{M}$. A meta-learner $S$ can then be trained to select the best algorithm for an instance $x$ based on its meta-features, that is, an algorithm $\alpha^*$ with maximum predictive performance for $x$ as given by $y$: 

\begin{equation} \label{eq:algo_selection}
	\alpha^* = S(f(x)) = \arg \max_{\alpha} ||y(\alpha, x)||.
\end{equation}

The ISA framework goes further to give insights into why some instances are harder to solve than others, using both the information of meta-features and algorithmic performance in a 2-D embedding, called Instance Space (IS), that can be visually inspected. An optimization problem is solved to find the mapping from meta-features to the IS $g(f(x))$, such that the distribution of algorithmic performance metrics and meta-features values display as close a linear trend as possible in the IS embedding. This embedding can then be inspected for regions of good and bad algorithmic performance and a new learner can be created to select new algorithms, as in:

\begin{equation}
	\alpha^* = S^\prime(g(f(x)))
\end{equation}

In the IS, it is also possible to define algorithm footprints $\varphi(y(\alpha, x))$, which are areas where an algorithm $\alpha$ is expected to perform well. A set of objective measures can be derived from these footprints and aid in the inference of algorithmic performance for other instances that were not in $\mathcal{I}$, such as: 

\begin{itemize}
	\item the area of the footprint $A$, which can be normalized across multiple algorithms for comparison;
	\item the density of the footprint $\rho$, which can be calculated as the ratio between the number of instances enclosed by the footprint and its area;
	\item the purity of the footprint $p$, which is the percentage of instances in the footprint that have good performance.
\end{itemize}

Summarizing, the application of ISA requires \cite{Munoz2018}:

\begin{enumerate}
	\item building the meta-dataset $\mathcal{M}$;
	\item reducing the set of meta-features in $\mathcal{M}$, keeping only those able to discriminate algorithmic performance;
	\item creating the 2-D IS from $\mathcal{M}$;
	\item building the algorithms' footprints in the IS.
\end{enumerate}

\subsection{Instance Space construction}

The problem of finding the optimal mapping between the instance metadata domain to a 2-D IS has been modelled using the Prediction Based Linear Dimensionality Reduction (PBLDR) method \cite{Munoz2018}. Given a meta-dataset $\mathcal{M}$ with $m$ meta-features, $n$ instances and $a$ algorithms, let $F \in \mathbb{R}^{m \times n}$ be the matrix containing the meta-features values for all instances and $Y \in \mathbb{R}^{n \ times a}$ be the matrix containing the performance measure of the algorithms on the same instances. The optimal projection is achieved by minimizing the MSE:

\begin{equation}
	MSE = ||\mathbf{F} - \mathbf{\hat{F}}||_F^2 + ||\mathbf{Y} - \mathbf{\hat{Y}}||_F^2,
\end{equation}

such that:

\begin{gather}
	\mathbf{Z} = \mathbf{A}_r \mathbf{F} \label{eq:z_is}, \\
	\mathbf{\hat{F}} = \mathbf{B}_r \mathbf{Z} \label{eq:f_hat}, \\
	\mathbf{\hat{Y}}^T = \mathbf{C}_r \mathbf{Z} \label{eq:y_hat}.
\end{gather}

Where $\mathbf{A}_r \in \mathbb{R}^{2 \times m}$, $\mathbf{B}_r \in \mathbb{R}^{m \times 2}$ and $\mathbf{C}_r \in \mathbb{R}^{a \times 2}$. $\mathbf{Z} \in \mathbb{R}^{2 \times n}$ is the matrix of instance coordinates in the IS and $A_r$ is the projection matrix. \cite{Lorena2022} solves this problem numerically using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm.

\subsection{Footprint analysis}

As said before, a footprint is a region in the IS where an algorithm is expected to perform well based on inference from empirical performance analysis \cite{Munoz2018}. \cite{Lorena2022} goes into more detail of its construction, qualitatively we can infer from the set of footprint measures defined before that a good algorithm will have large quantities of each of those measures, i.e. a large area shows that an algorithm performs well in a large portion of the IS, a large density shows that the footprint contains a large amount of instances and a large purity shows that the algorithm performs well in most instances.

\section{ISA for a single dataset}

In \cite{Munoz2018}, ISA is defined with an instance being an entire classification dataset. \cite{Lorena2022} has adapted the framework and reduced the problem space $\mathcal{P}$ into a single dataset, with each instance being an observation inside the dataset. This has come with the removal of some steps of the original ISA, namely the algorithmic recommendation module and the creation of new instances from the IS.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[node distance=3cm and 3cm, align=center, on grid, trim left]
		
		% Start node
		\node[draw, rounded corners, align=center] (dataset) {Original \\dataset $\mathcal{D}$};
		
		% Nodes to the right of dataset
		\node[draw, rounded corners, above right=of dataset, xshift=1cm] (hardness) {$f(\mathbf{x}_i) \in \mathcal{F}$ \\Hardness \\ measures};
		\node[draw, rounded corners, below right=of dataset, xshift=1cm] (algorithm) {$\alpha \in \mathcal{A}$ \\ Algorithm \\ space};
		
		% Arrows from dataset to hardness and algorithm
		\draw[->] (dataset.east) -- ++(0.6cm,0) |- node[near end, above] {$f$} (hardness.west);
		\draw[->] (dataset.east) -- ++(0.6cm,0) |- (algorithm.west);
		
		% Node to the right of hardness
		\node[draw, rounded corners, right=of hardness, xshift=1cm] (feature) {$f_s(f(\mathbf{x}_i), y)$ \\Feature \\ selection};
		\draw[->] (hardness.east) -- (feature.west);
		
		% Node to the right of algorithm
		\node[draw, rounded corners, right=of algorithm, xshift=1cm] (performance) {$y \in \mathcal{Y}$ \\Performance \\ space};
		\draw[->] (algorithm.east) -- (performance.west) node[midway, above] {$y(\alpha, \mathbf{x}_i)$};
		\draw[->] (performance.north) -- (feature.south);
		
		% Node to the right of feature and performance
		\node[draw, rounded corners, below right=of feature, align=center] (metadata) {Metadata \\ $\mathcal{M}$};
		\begin{scope}[tips=proper]
			\draw[->] (feature) -| ++(3cm,0) -- (metadata.north);
		\end{scope}
		
		\draw[->] (performance) -- ++(3cm,0) |- (metadata.south);
		
		% Node to the right of metadata
		\node[draw, rounded corners, right=of metadata, xshift=1cm] (instance) {$(z_1, z_2) \in \mathbb{R}^2$ \\Instance Space};
		\draw[->] (metadata.east) -- (instance.west) node[midway, above] {ISA};
		
	\end{tikzpicture}
	\caption{ISA framework for a single dataset. Extracted from \cite{Lorena2022}.} \label{fig:isa_dataset}
\end{figure}

Given a classification dataset $\mathcal{D}$ with $n_D$ instances $\mathbf{x}_i$, $m_D$ input features and each instance labelled in a class $y_i$, we have:

\begin{itemize}
	\item \textbf{Problem space} $\mathcal{P}$: reduced to the dataset $\mathcal{D}$;
	\item \textbf{Instance sub-space} $\mathcal{I}$: contains all individual instances $\mathbf{x}_i$;
	\item \textbf{Feature space} $\mathcal{F}$: contains a set of meta-features known as \emph{hardness measures}.
\end{itemize}

The modified framework is shown in figure \ref{fig:isa_dataset}. For each instance, a set of hardness measures are stored in $\mathcal{F}$, and each algorithm $\alpha \in \mathcal{A}$ is evaluated over the instance $\mathbf{x}_i$ and has its performance $y(\alpha, \mathbf{x}_i)$ measured and stored in $\mathcal{Y}$. The measure used is a cross-validated log-loss error obtained in the prediction of the instance label. A feature selection is used with these measures and the hardness measures, resulting in a reduced feature set $f_s(f(\mathbf{x}_i), y)$.

Combining the reduced feature set to the predictive performance of the algorithms allows us to construct the meta-dataset $\mathcal{M}$, from which the IS is extracted. The steps are explained in \cite{Lorena2022}, we will summarize some important topics.

\subsection{Hardness measures} \label{subsec:hardness_measures}

\cite{Lorena2022} revisits the definition of \emph{Instance Hardness} (IH), a property that indicates the probability of an instance being misclassified given a classification hypothesis:

\begin{equation} \label{eq:instance_hardness}
	IH_h(\mathbf{x}_i, y_i) = 1 - p(y_i | \mathbf{x}_i, h),
\end{equation}

where $h: X \rightarrow Y$ is a classification hypothesis mapping an input space of features $X$ to an output space of labels $Y$. In practice, $h$ is a learning algorithm induced from the dataset $\mathcal{D}$ and hyperparameters $\mathbf{\beta}$, that is $h = l(\mathcal{D}, \mathbf{\beta})$. We can then compute the IH of the whole algorithm space $\mathcal{A}$:

\begin{equation}
	IH_{\mathcal{A}}(\mathbf{x}_i, y_i) = 1 - \frac{1}{|\mathcal{A}|} \sum_{l \in \mathcal{A}} p(y_i | \mathbf{x}_i, l(\mathcal{D}, \mathbf{\beta})).
\end{equation}

The idea is that instances frequently misclassified over the algorithm space can be considered hard, and instances being correctly classified are considered easy. \cite{Lorena2022} also shows a set of \emph{hardness metrics} being extracted for each instance. Table \ref{tab:hardness_measures} lists the measures used, the formal definition for each one is shown in Appendix \ref{ch:instance_hardness}.

\begin{table}[ht]
	\centering
	\begin{tabular}{llll}
		\hline
		Measure                                      & Acronym & Min.        & Max.        \\ \hline
		k-Disagreeing Neighbors                      & $kDN$   & 0           & 1           \\
		Disjunct Class Percentage                    & $DCP$   & 0           & 1           \\
		Tree Depth (pruned)                          & $TD_P$  & 0           & 1           \\
		Tree Depth (unpruned)                        & $TD_U$  & 0           & 1           \\
		Class Likelihood                             & $CL$    & 0           & 1           \\
		Class Likelihood Difference                  & $CLD$   & 0           & 1           \\
		Fraction of features in overlapping areas    & $F1$    & 0           & 1           \\
		Fraction of nearby instances different class & $N1$    & 0           & 1           \\
		Ratio of intra-extra class distances         & $N2$    & 0           & $\approx 1$ \\
		Local set cardinality                        & $LSC$   & 0           & 1           \\
		Local set radius                             & $LSR$   & 0           & 1           \\
		Usefulness                                   & $U$     & $\approx 0$ & 1           \\
		Harmfulness                                  & $H$     & 0           & $\approx 1$ \\ \hline
	\end{tabular}
	\caption{Hardness measures used in \cite{Lorena2022}.} \label{tab:hardness_measures}
\end{table}

\newpage
\subsection{Performance assessment}

To assess the performance from each algorithm in the dataset $\mathcal{D}$, it is first split into $r$ folds according to the cross-validation strategy such that each instance from the dataset belongs to only one fold. $r-1$ folds are used for training the algorithm while the last fold is used for testing. Therefore we can compute a performance estimate for each instance and algorithm combination. Repeating this process yields an interval estimation \cite{Lorena2022}. The measure used for performance assessment was the log-loss, or cross-entropy, defined in equation \ref{eq:logloss}:

\begin{equation} \label{eq:logloss}
	\logloss(\mathbf{x}_i) = - \sum_{c=1}^{C} y_{i, c} \log(p_{i, c}),
\end{equation}

where $C$ is the number of classes the problem has, $y_{i, c}$ is a binary indicator of whether the class $c$ is an actual label of $\mathbf{x}_i$ or not and $p_{i, c}$ is the probability that the classifier attributes $\mathbf{x}_i$ to class $c$.

\subsection{Feature selection}

For feature selection, \cite{Lorena2022} employs a method called \emph{Minimum Redundancy Maximum Relevance} (MRMR), a criterion that gradually reduces the effect of feature redundancy as more features are selected. There is a trade-off between minimizing the number of redundant features and keeping relevant features, this method rejects redundant features at first but tolerates redundancy as more informative features are added. The mathematical model for MRMR is in equation \ref{eq:MRMR}:

\begin{equation} \label{eq:MRMR}
	J(\mathbf{f}_k) = MI(\mathbf{f}_k; \mathbf{y}_j) - \frac{1}{|\mathcal{S}_j|} \sum_{\mathbf{f}_i \in \mathcal{S}_j} MI(\mathbf{f}_k; \mathbf{f}_i),
\end{equation}

where $J(\mathbf{f}_k)$ is a score for the $k$-th feature vector $\mathbf{f}_k$, $ MI(\mathbf{f}_k; \mathbf{y}_j)$ is the mutual information between the feature vector and the response variable for the $j$-th algorithm and $\mathcal{S}_j$ is the set of selected features for the $j$-th algorithm. $\mathcal{S}_j$ is initially empty and the first feature chosen is the one with maximum mutual information between the feature vector and the response variable $\mathbf{y}_j$. Features are selected based on their score in subsequent rounds until there are a desired number of features chosen $n_f = |\mathcal{S}_j|$

\subsection{IS representation and footprints}

Given the now constructed meta-dataset $\mathcal{M}$ after feature selection and performance assessments, the 2-D IS can be constructed. \cite{Lorena2022} added a rotation to the IS so that bad instances are placed towards the upper left of the IS, while good instances are placed towards the bottom right.

All of this section is implemented in the Python library PyHard \cite{Lorena2022}. This work will use this implementation for reproducing this methodology.