@ARTICLE{Munoz2018,
	author={Mu{\~{n}}oz, Mario A.
	and Villanova, Laura
	and Baatar, Davaatseren
	and Smith-Miles, Kate},
	title={Instance spaces for machine learning classification},
	journal={Machine Learning},
	year={2018},
	month={Jan},
	day={01},
	volume={107},
	number={1},
	pages={109-147},
	issn={1573-0565},
	doi={10.1007/s10994-017-5629-5},
	url={https://doi.org/10.1007/s10994-017-5629-5}
}

@ARTICLE{RepLearning,
	
	author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	
	title={Representation Learning: A Review and New Perspectives}, 
	
	year={2013},
	
	volume={35},
	
	number={8},
	
	pages={1798-1828},
	
	doi={10.1109/TPAMI.2013.50}
}

@book{Goodfellow-et-al-2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@article{pandas,
author = {Mckinney, Wes},
year = {2011},
month = {01},
pages = {},
title = {pandas: a Foundational Python Library for Data Analysis and Statistics},
journal = {Python High Performance Science Computer}
}

@ARTICLE{Lorena2022,
author={Paiva, Pedro Yuri Arbs
and Moreno, Camila Castro
and Smith-Miles, Kate
and Valeriano, Maria Gabriela
and Lorena, Ana Carolina},
title={Relating instance hardness to classification performance in a dataset: a visual approach},
journal={Machine Learning},
year={2022},
month={Aug},
day={01},
volume={111},
number={8},
pages={3085-3123},
issn={1573-0565},
doi={10.1007/s10994-022-06205-9},
url={https://doi.org/10.1007/s10994-022-06205-9}
}

@INPROCEEDINGS{Goodfellow2014,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Generative Adversarial Nets},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
	volume = {27},
	year = {2014}
}

@MISC{paszke2019pytorch,
	title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
	author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
	year={2019},
	eprint={1912.01703},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://doi.org/10.48550/arXiv.1912.01703}
}

@MISC{kingma2017adam,
	title={Adam: A Method for Stochastic Optimization}, 
	author={Diederik P. Kingma and Jimmy Ba},
	year={2017},
	eprint={1412.6980},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://doi.org/10.48550/arXiv.1412.6980}
}

@ARTICLE{Puri2016,
	abstract = {An ANN is a computational model inspired by networks of biological neurons, wherein the neurons compute output values from inputs. It learns from its past experience and errors in a non-linear parallel processing manner. The neuron is the basic calculating entities which computes from a number of inputs and deliver one output comparing with threshold value and turned on (fired). The computational processing is done by internal structural arrangement consists of hidden layers which utilizes the back propagation and feed forward mechanism to deliver output close to accuracy. The learning is based on reinforcement (supervised) and unsupervised (no target) type. The unsupervised mimics the biological neuron pattern of learning. ANN is widely used in medicine as classifier in cancer applications, drug delivery, pharmaceutical research and Blood-Brain Barrier (BBB) permeability.},
	author = {Munish Puri and Aum Solanki and Timothy Padawer and Srinivas M. Tipparaju and Wilfrido Alejandro Moreno and Yashwant Pathak},
	doi = {10.1016/B978-0-12-801559-9.00001-6},
	isbn = {9780128015599},
	journal = {Artificial Neural Network for Drug Design, Delivery and Disposition},
	keywords = {ANN as classifier,ANN in Blood-Brain Barrier (BBB) permeability,ANN in medicine,Artificial Neural Network,Neural network in pharmaceutical},
	pages = {3-13},
	publisher = {Elsevier Inc.},
	title = {Introduction to Artificial Neural Network (ANN) as a Predictive Tool for Drug Design, Discovery, Delivery, and Disposition: Basic Concepts and Modeling. Basic Concepts and Modeling},
	year = {2016},
}

@ARTICLE{HORNIK1989,
	title = {Multilayer feedforward networks are universal approximators},
	journal = {Neural Networks},
	volume = {2},
	number = {5},
	pages = {359-366},
	year = {1989},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@misc{agarap2019deep,
      title={Deep Learning using Rectified Linear Units (ReLU)}, 
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
	  doi = {https://doi.org/10.48550/arXiv.1803.08375}
}

@INCOLLECTION{RICE1976,
	title = {The Algorithm Selection Problem},
	editor = {Morris Rubinoff and Marshall C. Yovits},
	series = {Advances in Computers},
	publisher = {Elsevier},
	volume = {15},
	pages = {65-118},
	year = {1976},
	issn = {0065-2458},
	doi = {https://doi.org/10.1016/S0065-2458(08)60520-3},
	url = {https://www.sciencedirect.com/science/article/pii/S0065245808605203},
	author = {John R. Rice},
}

@ARTICLE{Yuan2019,
	author={Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
	journal={IEEE Transactions on Neural Networks and Learning Systems}, 
	title={Adversarial Examples: Attacks and Defenses for Deep Learning}, 
	year={2019},
	volume={30},
	number={9},
	pages={2805-2824},
	doi={10.1109/TNNLS.2018.2886017}}

@Inbook{Adam2019,
	author="Adam, Stavros P.
	and Alexandropoulos, Stamatios-Aggelos N.
	and Pardalos, Panos M.
	and Vrahatis, Michael N.",
	editor="Demetriou, Ioannis C.
	and Pardalos, Panos M.",
	title="No Free Lunch Theorem: A Review",
	bookTitle="Approximation and Optimization : Algorithms, Complexity and Applications",
	year="2019",
	publisher="Springer International Publishing",
	address="Cham",
	pages="57--82",
	isbn="978-3-030-12767-1",
	doi="10.1007/978-3-030-12767-1_5",
	url="https://doi.org/10.1007/978-3-030-12767-1_5"
}

@article{Kolla2020,
	author = {Venkata Ravi Kiran Kolla},
	journal={International Journal of Computer Engineering and Technology},
	month = {12},
	title = {Paws And Reflect: A Comparative Study of Deep Learning Techniques For Cat Vs Dog Image Classification},
	url = {https://papers.ssrn.com/abstract=4413724},
	year = {2020},
}

@book{lorena2021inteligencia,
	title   = {Intelig{\^e}ncia artificial: uma abordagem de aprendizado de m{\'a}quina},
	author = {Faceli, Katti and Lorena, Ana Carolina and Gama, João and Carvalho, André Carlos Ponce de Leon Ferreira de},
	year = {2021},
	publisher = {LTC}
}

@article{SmithMiles2023,
	author = {Kate Smith-Miles and Mario Andrés Mu{\~{n}}oz},
	doi = {10.1145/3572895},
	issn = {0360-0300},
	issue = {12},
	journal = {ACM Computing Surveys},
	keywords = {Algorithm footprints,MATLAB,algorithm selection,benchmarking,meta-heuristics,meta-learning,software as a service,test instance diversity,timetabling},
	month = {3},
	pages = {1-31},
	publisher = {
	ACM
	New York, NY
	},
	title = {Instance Space Analysis for Algorithm Testing: Methodology and Software Tools},
	volume = {55},
	url = {https://dl.acm.org/doi/10.1145/3572895},
	year = {2023},
}

@article{Smith2014,
	abstract = {Most data complexity studies have focused on characterizing the complexity of the entire data set and do not provide information about individual instances. Knowing which instances are misclassified and understanding why they are misclassified and how they contribute to data set complexity can improve the learning process and could guide the future development of learning algorithms and data analysis methods. The goal of this paper is to better understand the data used in machine learning problems by identifying and analyzing the instances that are frequently misclassified by learning algorithms that have shown utility to date and are commonly used in practice. We identify instances that are hard to classify correctly (instance hardness) by classifying over 190,000 instances from 64 data sets with 9 learning algorithms. We then use a set of hardness measures to understand why some instances are harder to classify correctly than others. We find that class overlap is a principal contributor to instance hardness. We seek to integrate this information into the training process to alleviate the effects of class overlap and present ways that instance hardness can be used to improve learning.},
	author = {Michael R Smith and Tony Martinez and Christophe Giraud-Carrier},
	doi = {10.1007/s10994-013-5422-z},
	issn = {1573-0565},
	issue = {2},
	journal = {Machine Learning},
	pages = {225-256},
	title = {An instance level analysis of data complexity},
	volume = {95},
	url = {https://doi.org/10.1007/s10994-013-5422-z},
	year = {2014},
}