@ARTICLE{Munoz2018,
	author={Mu{\~{n}}oz, Mario A.
	and Villanova, Laura
	and Baatar, Davaatseren
	and Smith-Miles, Kate},
	title={Instance spaces for machine learning classification},
	journal={Machine Learning},
	year={2018},
	month={Jan},
	day={01},
	volume={107},
	number={1},
	pages={109-147},
	issn={1573-0565},
	doi={10.1007/s10994-017-5629-5},
	url={https://doi.org/10.1007/s10994-017-5629-5}
}

@ARTICLE{Lorena2022,
author={Paiva, Pedro Yuri Arbs
and Moreno, Camila Castro
and Smith-Miles, Kate
and Valeriano, Maria Gabriela
and Lorena, Ana Carolina},
title={Relating instance hardness to classification performance in a dataset: a visual approach},
journal={Machine Learning},
year={2022},
month={Aug},
day={01},
volume={111},
number={8},
pages={3085-3123},
issn={1573-0565},
doi={10.1007/s10994-022-06205-9},
url={https://doi.org/10.1007/s10994-022-06205-9}
}

@INPROCEEDINGS{Goodfellow2014,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Generative Adversarial Nets},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
	volume = {27},
	year = {2014}
}

@MISC{paszke2019pytorch,
	title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
	author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas KÃ¶pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
	year={2019},
	eprint={1912.01703},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://doi.org/10.48550/arXiv.1912.01703}
}

@ARTICLE{Puri2016,
	abstract = {An ANN is a computational model inspired by networks of biological neurons, wherein the neurons compute output values from inputs. It learns from its past experience and errors in a non-linear parallel processing manner. The neuron is the basic calculating entities which computes from a number of inputs and deliver one output comparing with threshold value and turned on (fired). The computational processing is done by internal structural arrangement consists of hidden layers which utilizes the back propagation and feed forward mechanism to deliver output close to accuracy. The learning is based on reinforcement (supervised) and unsupervised (no target) type. The unsupervised mimics the biological neuron pattern of learning. ANN is widely used in medicine as classifier in cancer applications, drug delivery, pharmaceutical research and Blood-Brain Barrier (BBB) permeability.},
	author = {Munish Puri and Aum Solanki and Timothy Padawer and Srinivas M. Tipparaju and Wilfrido Alejandro Moreno and Yashwant Pathak},
	doi = {10.1016/B978-0-12-801559-9.00001-6},
	isbn = {9780128015599},
	journal = {Artificial Neural Network for Drug Design, Delivery and Disposition},
	keywords = {ANN as classifier,ANN in Blood-Brain Barrier (BBB) permeability,ANN in medicine,Artificial Neural Network,Neural network in pharmaceutical},
	pages = {3-13},
	publisher = {Elsevier Inc.},
	title = {Introduction to Artificial Neural Network (ANN) as a Predictive Tool for Drug Design, Discovery, Delivery, and Disposition: Basic Concepts and Modeling. Basic Concepts and Modeling},
	year = {2016},
}

@ARTICLE{HORNIK1989,
	title = {Multilayer feedforward networks are universal approximators},
	journal = {Neural Networks},
	volume = {2},
	number = {5},
	pages = {359-366},
	year = {1989},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}